{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOsBZnKcmjMjVklZYd7tmAZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ahsan97Javed/gtfs-batch-pipeline/blob/main/preprocessing_gtfs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GTFS Batch Processing Pipeline â€” Preprocessing Microservice\n",
        "\n",
        "## 1. Mount Google Drive & Set Paths\n"
      ],
      "metadata": {
        "id": "_X0qGH75-u7H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_G7ZVct-iyZ",
        "outputId": "4ab3022f-c859-47c5-9e4b-8ac01709a6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "validated_path = '/content/drive/My Drive/GTFS_VALIDATED'\n",
        "cleaned_path = '/content/drive/My Drive/GTFS_CLEANED'\n",
        "os.makedirs(cleaned_path, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Load Validated GTFS Files\n"
      ],
      "metadata": {
        "id": "lVc7iVXb-32i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = os.listdir(validated_path)\n",
        "dfs = {}\n",
        "for fname in files:\n",
        "    if fname.endswith('.txt'):\n",
        "        fpath = os.path.join(validated_path, fname)\n",
        "        df = pd.read_csv(fpath)\n",
        "        dfs[fname] = df\n",
        "        print(f\"Loaded {fname}, shape: {df.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12OwyIKM-1zg",
        "outputId": "07d9d5bf-65f9-4746-ffbc-5d9895e0e8b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded calendar.txt, shape: (5553, 10)\n",
            "Loaded feed_info.txt, shape: (1, 8)\n",
            "Loaded stops.txt, shape: (677435, 6)\n",
            "Loaded routes.txt, shape: (25081, 5)\n",
            "Loaded calendar_dates.txt, shape: (13229, 3)\n",
            "Loaded agency.txt, shape: (451, 5)\n",
            "Loaded trips.txt, shape: (1618937, 3)\n",
            "Loaded attributions.txt, shape: (3, 8)\n",
            "Loaded stop_times.txt, shape: (32966240, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Cleaning & Preprocessing\n",
        "\n",
        "- Handle missing values\n",
        "- Standardize column types\n",
        "- Normalize timestamps/time columns\n",
        "- Remove duplicates\n",
        "- Ensure data consistency\n"
      ],
      "metadata": {
        "id": "L04-hDiX_BfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Preprocessing per GTFS file\n",
        "for fname, df in dfs.items():\n",
        "    print(f\"\\nCleaning {fname}:\")\n",
        "\n",
        "    # Remove exact duplicates\n",
        "    before = df.shape[0]\n",
        "    df = df.drop_duplicates()\n",
        "    after = df.shape[0]\n",
        "    print(f\" - Dropped {before - after} duplicate rows.\")\n",
        "\n",
        "    # Remove empty rows (all NA or blank)\n",
        "    before = df.shape[0]\n",
        "    df = df.dropna(how='all')\n",
        "    after = df.shape[0]\n",
        "    print(f\" - Dropped {before - after} fully empty rows.\")\n",
        "\n",
        "    # Strip whitespace from all string columns\n",
        "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "\n",
        "    # Custom cleaning by file type\n",
        "    if fname == 'stops.txt':\n",
        "        # Remove rows with missing stop_id\n",
        "        before = df.shape[0]\n",
        "        df = df[~df['stop_id'].isna() & (df['stop_id'] != \"\")]\n",
        "        after = df.shape[0]\n",
        "        print(f\" - Removed {before - after} rows missing required stop_id.\")\n",
        "        # Enforce float type for lat/lon, set to NaN if invalid\n",
        "        for col in ['stop_lat', 'stop_lon']:\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "        # Replace missing names with 'Unknown Stop'\n",
        "        df['stop_name'] = df['stop_name'].replace('', 'Unknown Stop').fillna('Unknown Stop')\n",
        "        # Fill optional columns with blank\n",
        "        for col in ['parent_station', 'location_type']:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna('').replace(np.nan, '', regex=True)\n",
        "\n",
        "    elif fname == 'routes.txt':\n",
        "        # Remove rows with missing route_id\n",
        "        before = df.shape[0]\n",
        "        df = df[~df['route_id'].isna() & (df['route_id'] != \"\")]\n",
        "        after = df.shape[0]\n",
        "        print(f\" - Removed {before - after} rows missing required route_id.\")\n",
        "        # Standardize route_type as integer, set invalid to -1\n",
        "        if 'route_type' in df.columns:\n",
        "            df['route_type'] = pd.to_numeric(df['route_type'], errors='coerce').fillna(-1).astype(int)\n",
        "        # Fill optional columns with blank\n",
        "        for col in ['route_short_name', 'agency_id', 'route_long_name']:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna('').replace(np.nan, '', regex=True)\n",
        "\n",
        "    elif fname == 'stop_times.txt':\n",
        "        # Remove rows with missing trip_id or stop_id\n",
        "        before = df.shape[0]\n",
        "        df = df[~df['trip_id'].isna() & (df['trip_id'] != \"\") & ~df['stop_id'].isna() & (df['stop_id'] != \"\")]\n",
        "        after = df.shape[0]\n",
        "        print(f\" - Removed {before - after} rows missing trip_id or stop_id.\")\n",
        "        # Standardize and pad time columns to HH:MM:SS\n",
        "        for col in ['arrival_time', 'departure_time']:\n",
        "            if col in df.columns:\n",
        "                # GTFS allows times >24:00 (use as string)\n",
        "                df[col] = df[col].astype(str).apply(\n",
        "                    lambda x: x if pd.isna(x) or x == '' else\n",
        "                    ':'.join([i.zfill(2) for i in x.split(\":\")])\n",
        "                )\n",
        "        # Fill NA for pickup/drop_off types with '0'\n",
        "        for col in ['pickup_type', 'drop_off_type']:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna('0').replace('', '0')\n",
        "\n",
        "    elif fname == 'trips.txt':\n",
        "        # Remove rows with missing trip_id\n",
        "        before = df.shape[0]\n",
        "        df = df[~df['trip_id'].isna() & (df['trip_id'] != \"\")]\n",
        "        after = df.shape[0]\n",
        "        print(f\" - Removed {before - after} rows missing required trip_id.\")\n",
        "        # Fill missing route_id/service_id with blank\n",
        "        for col in ['route_id', 'service_id']:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna('').replace(np.nan, '', regex=True)\n",
        "\n",
        "    elif fname == 'agency.txt':\n",
        "        # Remove rows with missing agency_id\n",
        "        before = df.shape[0]\n",
        "        df = df[~df['agency_id'].isna() & (df['agency_id'] != \"\")]\n",
        "        after = df.shape[0]\n",
        "        print(f\" - Removed {before - after} rows missing required agency_id.\")\n",
        "        # Fill optional columns with blank\n",
        "        for col in ['agency_name', 'agency_url', 'agency_timezone', 'agency_lang']:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna('').replace(np.nan, '', regex=True)\n",
        "\n",
        "    elif fname == 'calendar.txt':\n",
        "        # Remove rows missing service_id\n",
        "        before = df.shape[0]\n",
        "        df = df[~df['service_id'].isna() & (df['service_id'] != \"\")]\n",
        "        after = df.shape[0]\n",
        "        print(f\" - Removed {before - after} rows missing required service_id.\")\n",
        "        # Days of week as int 0/1\n",
        "        for day in ['monday','tuesday','wednesday','thursday','friday','saturday','sunday']:\n",
        "            if day in df.columns:\n",
        "                df[day] = pd.to_numeric(df[day], errors='coerce').fillna(0).astype(int)\n",
        "        # Dates as string or int\n",
        "        for col in ['start_date','end_date']:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].astype(str).str[:8]  # keep as YYYYMMDD\n",
        "\n",
        "    elif fname == 'calendar_dates.txt':\n",
        "        # Remove rows missing service_id or date\n",
        "        before = df.shape[0]\n",
        "        df = df[~df['service_id'].isna() & (df['service_id'] != \"\") & ~df['date'].isna() & (df['date'] != \"\")]\n",
        "        after = df.shape[0]\n",
        "        print(f\" - Removed {before - after} rows missing required service_id or date.\")\n",
        "        # exception_type as int 1/2\n",
        "        if 'exception_type' in df.columns:\n",
        "            df['exception_type'] = pd.to_numeric(df['exception_type'], errors='coerce').fillna(1).astype(int)\n",
        "        # Dates as string or int\n",
        "        if 'date' in df.columns:\n",
        "            df['date'] = df['date'].astype(str).str[:8]\n",
        "\n",
        "    elif fname == 'feed_info.txt':\n",
        "        # Fill missing publisher name/url with 'Unknown'\n",
        "        for col in ['feed_publisher_name', 'feed_publisher_url']:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].replace('', 'Unknown').fillna('Unknown')\n",
        "        # Dates as string\n",
        "        for col in ['feed_start_date','feed_end_date']:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].astype(str).str[:8]\n",
        "        # Fill other missing fields with blank\n",
        "        for col in df.columns:\n",
        "            df[col] = df[col].fillna('').replace(np.nan, '', regex=True)\n",
        "\n",
        "    elif fname == 'attributions.txt':\n",
        "        # Fill required columns with 'Unknown' if missing\n",
        "        for col in ['organization_name']:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].replace('', 'Unknown').fillna('Unknown')\n",
        "        # Fill optional boolean columns with 0\n",
        "        for col in ['is_producer','is_operator','is_authority']:\n",
        "            if col in df.columns:\n",
        "                df[col] = df[col].fillna('0').replace('', '0')\n",
        "        # Fill other missing fields with blank\n",
        "        for col in df.columns:\n",
        "            df[col] = df[col].fillna('').replace(np.nan, '', regex=True)\n",
        "    else:\n",
        "        # For any non-standard file: fillna with blank, drop dups, strip whitespace\n",
        "        df = df.drop_duplicates()\n",
        "        df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
        "        df = df.fillna('')\n",
        "        print(\" - Cleaned generic GTFS extension file.\")\n",
        "\n",
        "    dfs[fname] = df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A-ni_eDn-_LF",
        "outputId": "7429e055-37d3-4308-a50d-bd2db6fa94c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cleaning calendar.txt:\n",
            " - Dropped 0 duplicate rows.\n",
            " - Dropped 0 fully empty rows.\n",
            " - Removed 0 rows missing required service_id.\n",
            "\n",
            "Cleaning feed_info.txt:\n",
            " - Dropped 0 duplicate rows.\n",
            " - Dropped 0 fully empty rows.\n",
            "\n",
            "Cleaning stops.txt:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-2447458014.py:20: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-4-2447458014.py:20: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - Dropped 0 duplicate rows.\n",
            " - Dropped 0 fully empty rows.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-2447458014.py:20: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - Removed 0 rows missing required stop_id.\n",
            "\n",
            "Cleaning routes.txt:\n",
            " - Dropped 0 duplicate rows.\n",
            " - Dropped 0 fully empty rows.\n",
            " - Removed 0 rows missing required route_id.\n",
            "\n",
            "Cleaning calendar_dates.txt:\n",
            " - Dropped 0 duplicate rows.\n",
            " - Dropped 0 fully empty rows.\n",
            " - Removed 0 rows missing required service_id or date.\n",
            "\n",
            "Cleaning agency.txt:\n",
            " - Dropped 0 duplicate rows.\n",
            " - Dropped 0 fully empty rows.\n",
            " - Removed 0 rows missing required agency_id.\n",
            "\n",
            "Cleaning trips.txt:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-2447458014.py:20: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
            "/tmp/ipython-input-4-2447458014.py:20: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - Dropped 0 duplicate rows.\n",
            " - Dropped 0 fully empty rows.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-2447458014.py:20: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - Removed 0 rows missing required trip_id.\n",
            "\n",
            "Cleaning attributions.txt:\n",
            " - Dropped 0 duplicate rows.\n",
            " - Dropped 0 fully empty rows.\n",
            "\n",
            "Cleaning stop_times.txt:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4-2447458014.py:20: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " - Dropped 0 duplicate rows.\n",
            " - Dropped 0 fully empty rows.\n",
            " - Removed 0 rows missing trip_id or stop_id.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Save Cleaned Data"
      ],
      "metadata": {
        "id": "FCyqDfu2BxBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for fname, df in dfs.items():\n",
        "    df.to_csv(os.path.join(cleaned_path, fname), index=False)\n",
        "print(f\"All cleaned files saved to {cleaned_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3VaYGoS6AqB6",
        "outputId": "1fa6be36-03ae-4c5e-a9ec-a123d167621f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All cleaned files saved to /content/drive/My Drive/GTFS_CLEANED\n"
          ]
        }
      ]
    }
  ]
}
